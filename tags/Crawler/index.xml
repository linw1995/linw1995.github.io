<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Crawler on linw1995</title>
    <link>https://linw1995.com/tags/Crawler/</link>
    <description>Recent content in Crawler on linw1995</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 09 Apr 2021 00:08:00 +0800</lastBuildDate><atom:link href="https://linw1995.com/tags/Crawler/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>中间人与自动化</title>
      <link>https://linw1995.com/blog/%E4%B8%AD%E9%97%B4%E4%BA%BA%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96/</link>
      <pubDate>Fri, 09 Apr 2021 00:08:00 +0800</pubDate>
      
      <guid>https://linw1995.com/blog/%E4%B8%AD%E9%97%B4%E4%BA%BA%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96/</guid>
      <description>前言 自动化控制 App 爬取数据 实现  中间人代理配置 Android 配置 抓包分析 mitmproxy 插件功能 自动化控制 集成  中间人与自动化  
前言 自动化控制的技术一般是来做 App 测试的，但对于爬虫工程师来说，这项技术也可以用来爬取数据。
正常来说，爬虫工程师开发爬虫，首先要对 App 进行抓包分析，分析其功能所用的服务端接口。 找出调用接口所需要的方式及所支持的参数。之后就可以愉快地构造接口请求，对其服务端接口发起请求，获取我们所需要的数据。
但如今的 App 厂商往往为了避免这些服务端接口被恶意利用，会使用很多种方法或者技术去防止这种情况的发生。
比如在接口层面上，添加签名及时间戳参数。 添加签名参数的原因，一是防止参数被中间人修改，二是防止接口被调用； 那添加时间戳参数就是为了避免同样的参数被反复调用。
或者对请求及响应的内容进行加密。这样抓包看到的都是密文，接口分析工作就无从下手了。
在网络通信应用层面上，可以替换掉通信协议。这样连抓包都不好抓包了，不是熟悉的 HTTP 协议…
这些方法一般都只能通过对 App 进行逆向分析，找出加签加密算法；找出负责网络通信的模块。 再逆向出算法或协议来，或者沙盒去直接调用它。

自动化控制 App 爬取数据 但逆向是真的麻烦…这时候，自动化控制就派上用场了～
 本博客以 Android App 为例子来解释说明。但实际上这个方法可以用在各种场景，无论是通过浏览器去访问某个网页，IOS App，甚至是小程序…
 如果所需要的数据展示在 Android App 界面上，就可以通过自动化技术控制 App ，再解析界面的 XML 数据 1，直接获取到这些数据。
如果所需要的数据没有展示，自动化控制技术再配合上中间人抓包，就能获取到这些数据了。 这种情况很常见，服务端接口返回数据的结构是展示的超集（为了复用嘛，也不是 App 的服务端接口都会用上 GraphQL 2 的）。</description>
    </item>
    
    <item>
      <title>我是怎么开发一个数据提取轮子 DataExtractor</title>
      <link>https://linw1995.com/blog/%E6%88%91%E6%98%AF%E6%80%8E%E4%B9%88%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%E8%BD%AE%E5%AD%90-DataExtractor/</link>
      <pubDate>Tue, 16 Mar 2021 01:30:00 +0800</pubDate>
      
      <guid>https://linw1995.com/blog/%E6%88%91%E6%98%AF%E6%80%8E%E4%B9%88%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%E8%BD%AE%E5%AD%90-DataExtractor/</guid>
      <description>前言 理想中轮子的模样 实现  数据格式及其查询表达式 实现提取器，适配不同的场景 复杂的数据提取场景  轮子们功能对比 展望  
前言 数据提取对于爬虫开发来说可占了不少开发时间，是其中及其重要的一部分。 为了降低这方面的时间和心智成本，我调研且使用了不少轮子。
 像使用 XPath 作为选择器的 XSLT 1，对应 Python 的轮子就是最经典且好用的 lxml 2； 像在 Scrapy 中使用的的 Selectors 3，来自 Parsel 4; 像 Ruia 内置的 Data Item 5； 像到现在刚从 Scrapy 中重构独立出来的 Item Loaders 6 ，来自 itemloaders 7；  这些轮子有的很久远，比如 XSLT 1，只支持文本 to 文本的转换。
其间 Scrapy 的 Parsel 支持用 CSS Selectors 8 及 XPath 9 去对 HTML(XML) 10 文本数据进行提取。 而 itemloaders 通过更高层的抽象，基于 Parsel 定义提取器类，去对文本进行复杂的数据提取， 支持嵌套提取（不支持类定以嵌套，支持函数调用式去提取），还支持对原始数据进行格式转换等处理。</description>
    </item>
    
    <item>
      <title>网站观测工具</title>
      <link>https://linw1995.com/blog/%E7%BD%91%E7%AB%99%E8%A7%82%E6%B5%8B%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Tue, 10 Mar 2020 01:15:00 +0800</pubDate>
      
      <guid>https://linw1995.com/blog/%E7%BD%91%E7%AB%99%E8%A7%82%E6%B5%8B%E5%B7%A5%E5%85%B7/</guid>
      <description>通过 Chrome 本身的支持及集成 mitmproxy 这一伟大的工具对一个网站观测（爬取分析），做到开箱即用，完全沙盒，全程中间人抓包。
环境配置  安装 chrome
这就不需要多说什么了吧。如果是 MacOS 安装的 Chrome，其 CLI 路径为 /Applications/Google Chrome.app/Contents/MacOS/Google Chrome
 安装 mitm_chrome
CLI 工具，整合了 mitmproxy 与 Chrome 。能十分方便地快速启动，即开即用。
可以通过 pip 安装
pip install git+https://github.com/linw1995/mitm_chrome.git@master 证书安装
参考 mitmproxy 的文档安装一下，About Certificates | mitmproxy
  使用姿势 在 MacOS 上
mitm_chrome --chrome-path=&amp;#39;/Applications/Google Chrome.app/Contents/MacOS/Google Chrome&amp;#39; mitmproxy 在有 chrome 命令情况下，无需指定 --chrome-path 参数
mitm_chrome mitmproxy 后记 目前，我发现 Chrome 无法通过 CLI 去指定使用自定的证书，只能使用预先安装好的证书。应该有办法的，不然 seleniumwire 项目怎么做到抓包的，这个以后有时间再研究研究吧。</description>
    </item>
    
    <item>
      <title>如何在无头模式下的谷歌浏览器设置地理位置</title>
      <link>https://linw1995.com/blog/%E5%A6%82%E4%BD%95%E5%9C%A8%E6%97%A0%E5%A4%B4%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84%E8%B0%B7%E6%AD%8C%E6%B5%8F%E8%A7%88%E5%99%A8%E8%AE%BE%E7%BD%AE%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE/</link>
      <pubDate>Fri, 28 Feb 2020 23:59:59 +0800</pubDate>
      
      <guid>https://linw1995.com/blog/%E5%A6%82%E4%BD%95%E5%9C%A8%E6%97%A0%E5%A4%B4%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84%E8%B0%B7%E6%AD%8C%E6%B5%8F%E8%A7%88%E5%99%A8%E8%AE%BE%E7%BD%AE%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE/</guid>
      <description> 谷歌到很多方法。试了一个又一个，但大部分方法都过时了。所幸最后发现了个解决办法，就是用 Chrome Devtools Protocol。
以下是一个简单的例子，用最常用的 selenium 来执行 Chrome Devtools Protocol 命令。
import time from selenium.webdriver import Chrome, ChromeOptions options = ChromeOptions() options.add_argument(&amp;#34;--headless&amp;#34;) driver = Chrome(options=options) driver.execute_cdp_cmd( &amp;#34;Browser.grantPermissions&amp;#34;, { &amp;#34;origin&amp;#34;: &amp;#34;https://www.openstreetmap.org/&amp;#34;, &amp;#34;permissions&amp;#34;: [&amp;#34;geolocation&amp;#34;] }, ) driver.execute_cdp_cmd( &amp;#34;Emulation.setGeolocationOverride&amp;#34;, { &amp;#34;latitude&amp;#34;: 35.689487, &amp;#34;longitude&amp;#34;: 139.691706, &amp;#34;accuracy&amp;#34;: 100, }, ) driver.get(&amp;#34;https://www.openstreetmap.org/&amp;#34;) driver.find_element_by_xpath(&amp;#34;//span[@class=&amp;#39;icon geolocate&amp;#39;]&amp;#34;).click() time.sleep(3) # wait for the page full loaded driver.get_screenshot_as_file(&amp;#34;screenshot.png&amp;#34;) 
Refecences  Browser.grantPermissions | Chrome Devtools Protocol Emulation.setGeolocationOverride | Chrome Devtools Protocol  </description>
    </item>
    
    <item>
      <title>Scrapy 的并发处理</title>
      <link>https://linw1995.com/blog/Scrapy-%E7%9A%84%E5%B9%B6%E5%8F%91%E5%A4%84%E7%90%86/</link>
      <pubDate>Mon, 29 Jul 2019 23:59:59 +0000</pubDate>
      
      <guid>https://linw1995.com/blog/Scrapy-%E7%9A%84%E5%B9%B6%E5%8F%91%E5%A4%84%E7%90%86/</guid>
      <description>前言 看大型项目的源码时，经常摸不着头脑，耗时但收益少，所以也就不容易坚持下去。但如果带着问题来看的话，屏蔽掉问题无关的代码，专注于相关的，看看能不能真正的从源码中学习到好东西
问题: Scrapy 的并发处理 在 Scrapy 的文档中提到可以通过配置 DOWNLOAD_DELAY 来控制请求（request）间的时间间隔，配置 CONCURRENT_REQUESTS 来限制请求（request）的并发数（粒度可基于域名，或者 IP 地址）。甚至可以通过使用 Auto-Throttling 自动节流插件，来自动地且动态的调整这些配置项。
这样我们就有了以下这两个问题
 如何并发地处理请求 如何动态地改变配置项  以下的源码分析基于版本号为 1.7.2 的 scrapy 项目
如何并发地处理请求 scrapy/core/downloader/__init__.pyL138-L160 里的 Downloader 类的类方法 _process_queue 就负责处理 scrapy 的并发请求
为了更好的理解这函数的代码，首先要确定什么情况下 _process_queue 才会被调用
 当新的请求进入队列时 L135 当请求完成网络处理时 L186 当不满足 DONWLOAD_DELAY 时的延迟调用 L148  以下就是该函数的代码，加上了些注释方便大家理解
def _process_queue(self, spider, slot): if slot.latercall and slot.latercall.active(): # 由于同个 slot 会被 _process_queue 重复调用多次，情况 1，2 # 当情况 3 在处理时，则不执行 return # Delay queue processing if a download_delay is configured now = time() # 获取下载延迟，若是启用了 randomize_delay 则每次会获取到不同的下载延迟 delay = slot.</description>
    </item>
    
    <item>
      <title>用多线程加速爬虫的 lxml 解析</title>
      <link>https://linw1995.com/blog/%E7%94%A8%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%8A%A0%E9%80%9F%E7%88%AC%E8%99%AB%E7%9A%84-lxml-%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Wed, 17 Jul 2019 23:59:59 +0000</pubDate>
      
      <guid>https://linw1995.com/blog/%E7%94%A8%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%8A%A0%E9%80%9F%E7%88%AC%E8%99%AB%E7%9A%84-lxml-%E8%A7%A3%E6%9E%90/</guid>
      <description>当编写一个爬虫时，会使用 lxml 库来解析 HTML 文件。当爬取到了一个超大且复杂的 HTML 文件，解析起来十分耗费时间，进而影响了爬虫的正常运行
为了不影响爬虫的正常运行，尝试把解析任务交给线程池来处理
模拟爬虫 先随便准备一个 html 文件，就直接从 lxml 的文档上下载一个
wget https://lxml.de/tutorial.html -O example.html 举一个重解析的模拟爬虫例子，每次请求的 io 时间假定为 0.01s ，请求并发数无上限，总量为 2000 个。
import asyncio import time from pathlib import Path from lxml import html _html_text = None _latest_fetched = 0 async def fetch_text(): # 模拟爬取 await asyncio.sleep(0.01) # 记录最后的一个请求的结束时间 global _latest_fetched _latest_fetched = time.perf_counter() return _html_text def get_title(text): doc = html.fromstring(text) return doc.xpath(&amp;#34;//title/text()&amp;#34;)[0] async def create_task(): text = await fetch_text() title = get_title(text) return title async def main(): futs = [] for _ in range(2000): fut = asyncio.</description>
    </item>
    
  </channel>
</rss>
